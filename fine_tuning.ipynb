{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46d2304b-38f7-4a96-bfea-15b1433042a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:25:10.397123Z",
     "iopub.status.busy": "2025-07-10T03:25:10.396765Z",
     "iopub.status.idle": "2025-07-10T03:25:17.900828Z",
     "shell.execute_reply": "2025-07-10T03:25:17.900325Z",
     "shell.execute_reply.started": "2025-07-10T03:25:10.397097Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 03:25:15.350592: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from accelerate import Accelerator, DataLoaderConfiguration\n",
    "pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "594ea6d4-80ad-44b3-93c0-97ffb73d772e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T06:51:40.158639Z",
     "iopub.status.busy": "2025-07-09T06:51:40.158130Z",
     "iopub.status.idle": "2025-07-09T06:51:40.161670Z",
     "shell.execute_reply": "2025-07-09T06:51:40.161224Z",
     "shell.execute_reply.started": "2025-07-09T06:51:40.158616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n",
      "4.52.4\n"
     ]
    }
   ],
   "source": [
    "import accelerate, transformers\n",
    "print(accelerate.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306206b6-2e25-49ab-831b-b95d05a7ee27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:25:17.902103Z",
     "iopub.status.busy": "2025-07-10T03:25:17.901661Z",
     "iopub.status.idle": "2025-07-10T03:25:21.028293Z",
     "shell.execute_reply": "2025-07-10T03:25:21.027732Z",
     "shell.execute_reply.started": "2025-07-10T03:25:17.902082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6753f7c2-73c8-4974-9db3-e8e2dd68c1c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:25:21.029157Z",
     "iopub.status.busy": "2025-07-10T03:25:21.028896Z",
     "iopub.status.idle": "2025-07-10T03:25:34.529385Z",
     "shell.execute_reply": "2025-07-10T03:25:34.528854Z",
     "shell.execute_reply.started": "2025-07-10T03:25:21.029139Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\", torch_dtype=torch.bfloat16)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e75de9b-6777-40d8-8270-1ec2250fe598",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:25:39.785021Z",
     "iopub.status.busy": "2025-07-10T03:25:39.784244Z",
     "iopub.status.idle": "2025-07-10T03:25:39.790871Z",
     "shell.execute_reply": "2025-07-10T03:25:39.790270Z",
     "shell.execute_reply.started": "2025-07-10T03:25:39.784988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 406290432, \n",
      " Total Trainable Parameters are: 406290432\n"
     ]
    }
   ],
   "source": [
    "def number_of_trainable_parameters(model):\n",
    "    total_model_parameters = 0\n",
    "    total_trainable_parametrs = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        total_model_parameters += param.numel()\n",
    "        if param.requires_grad:\n",
    "            total_trainable_parametrs += param.numel()\n",
    "    return f\"Total Parameters: {total_model_parameters}, \\n Total Trainable Parameters are: {total_trainable_parametrs}\"\n",
    "\n",
    "print(number_of_trainable_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "728f9e2e-9d84-4fd5-b64d-f5c9e0c05f2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:25:40.650946Z",
     "iopub.status.busy": "2025-07-10T03:25:40.650602Z",
     "iopub.status.idle": "2025-07-10T03:25:40.656958Z",
     "shell.execute_reply": "2025-07-10T03:25:40.656435Z",
     "shell.execute_reply.started": "2025-07-10T03:25:40.650923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      " summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      " topic: upgrading system\n"
     ]
    }
   ],
   "source": [
    "dialogue = dataset['test'][200]['dialogue']\n",
    "summary = dataset['test'][200]['summary']\n",
    "topic = dataset['test'][200]['topic']\n",
    "\n",
    "print(f\"dialogue: {dialogue}\\n summary: {summary}\\n topic: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e9d6f6-9acb-4f5e-863a-a1ba97a8fd81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:25:42.499884Z",
     "iopub.status.busy": "2025-07-10T03:25:42.499513Z",
     "iopub.status.idle": "2025-07-10T03:25:50.854963Z",
     "shell.execute_reply": "2025-07-10T03:25:50.854393Z",
     "shell.execute_reply.started": "2025-07-10T03:25:42.499861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Prompt: \n",
      "Summarize the following\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      " Human Summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      " Model Summary - Zero Shot: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. You might also want to add a CD-ROM drive too, because most new software programs are coming out on Cds. It would allow you to make up your own flyers and banners for advertising.\n"
     ]
    }
   ],
   "source": [
    "# Zero shot Inference\n",
    "prompt = f\"\"\"\n",
    "Summarize the following\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs_to_encoder = tokenizer(prompt, dialogue, return_tensors=\"pt\")\n",
    "output_from_decoder = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs_to_encoder[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(f\"Input Prompt: {prompt}\\n Human Summary: {summary}\\n Model Summary - Zero Shot: {output_from_decoder}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d629c90-d4e9-4e7d-8bbf-8bc763e65770",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:25:50.856211Z",
     "iopub.status.busy": "2025-07-10T03:25:50.855938Z",
     "iopub.status.idle": "2025-07-10T03:25:58.610756Z",
     "shell.execute_reply": "2025-07-10T03:25:58.610300Z",
     "shell.execute_reply.started": "2025-07-10T03:25:50.856192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf854fc41cb443dac1ce17e8570afd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e1d8d68a6c44959706b587e2aa14f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88708684d39f49b48fc8780c6f7c7e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(batch):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in batch[\"dialogue\"]]\n",
    "    batch['input_ids'] = tokenizer(prompt, padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\").input_ids\n",
    "    batch['labels'] = tokenizer(batch[\"summary\"], padding=\"max_length\", max_length=512,truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return batch\n",
    "\n",
    "tokenized_training_datasets = dataset.map(preprocess, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "#tokenized_training_datasets =tokenized_training_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary'])\n",
    "tokenized_training_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af9ac075-19a8-40ef-8c15-ac16760d304d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:26:06.601206Z",
     "iopub.status.busy": "2025-07-10T03:26:06.600537Z",
     "iopub.status.idle": "2025-07-10T03:26:06.608065Z",
     "shell.execute_reply": "2025-07-10T03:26:06.607507Z",
     "shell.execute_reply.started": "2025-07-10T03:26:06.601178Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = \"/tmp/finetuned_model_checkpoints\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "676ab174-4bab-4e88-90c7-8797565667f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:39:04.595857Z",
     "iopub.status.busy": "2025-07-10T03:39:04.595449Z",
     "iopub.status.idle": "2025-07-10T03:39:04.610260Z",
     "shell.execute_reply": "2025-07-10T03:39:04.609746Z",
     "shell.execute_reply.started": "2025-07-10T03:39:04.595832Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_training_datasets['train'],\n",
    "    eval_dataset=tokenized_training_datasets['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af25e5ed-75d9-41c9-bd51-4ea9d7f8912a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:39:05.521346Z",
     "iopub.status.busy": "2025-07-10T03:39:05.521036Z",
     "iopub.status.idle": "2025-07-10T03:39:05.528532Z",
     "shell.execute_reply": "2025-07-10T03:39:05.528009Z",
     "shell.execute_reply.started": "2025-07-10T03:39:05.521325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'labels', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(\n",
    "    tokenized_training_datasets[\"train\"],\n",
    "    batch_size=2,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "print(batch.keys())  # Should only include: input_ids, attention_mask, labels, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abd2b24a-76a4-4ebb-93e0-89b6ef98ac58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T02:46:21.866491Z",
     "iopub.status.busy": "2025-07-09T02:46:21.866159Z",
     "iopub.status.idle": "2025-07-09T02:48:24.687778Z",
     "shell.execute_reply": "2025-07-09T02:48:24.687186Z",
     "shell.execute_reply.started": "2025-07-09T02:46:21.866469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=11.0, metrics={'train_runtime': 121.812, 'train_samples_per_second': 0.066, 'train_steps_per_second': 0.008, 'total_flos': 8668418408448.0, 'train_loss': 11.0, 'epoch': 0.0006418485237483953})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "65a919cb-afcd-4ab7-a3ca-821903e986aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:55:25.632768Z",
     "iopub.status.busy": "2025-07-10T03:55:25.632355Z",
     "iopub.status.idle": "2025-07-10T03:55:25.969822Z",
     "shell.execute_reply": "2025-07-10T03:55:25.969147Z",
     "shell.execute_reply.started": "2025-07-10T03:55:25.632740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "/dev/nvme1n1    5.0G  1.3G  3.8G  26% /home/sagemaker-user\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#Check disk space if it fails\n",
    "!df -h ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "47cb623f-0a0b-497d-958d-e4540872e208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:55:26.512635Z",
     "iopub.status.busy": "2025-07-10T03:55:26.512272Z",
     "iopub.status.idle": "2025-07-10T03:55:27.462379Z",
     "shell.execute_reply": "2025-07-10T03:55:27.461753Z",
     "shell.execute_reply.started": "2025-07-10T03:55:26.512607Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#And remove unnecessary files\n",
    "!rm -rf /home/sagemaker-user/*checkpoint*\n",
    "!rm -rf /home/sagemaker-user/output/*\n",
    "!rm -rf /home/sagemaker-user/.cache/huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "20ae885f-8f24-4e0c-8a73-f605573a59bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T04:16:23.286145Z",
     "iopub.status.busy": "2025-07-10T04:16:23.285501Z",
     "iopub.status.idle": "2025-07-10T04:16:23.289938Z",
     "shell.execute_reply": "2025-07-10T04:16:23.289431Z",
     "shell.execute_reply.started": "2025-07-10T04:16:23.286114Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "def upload_dir_to_s3(local_dir, bucket, s3_prefix):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    for root, _, files in os.walk(local_dir):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(full_path, local_dir)\n",
    "            s3_key = os.path.join(s3_prefix, rel_path)\n",
    "            print(f\"Uploading {full_path} to s3://{bucket}/{s3_key}\")\n",
    "            s3.upload_file(full_path, bucket, s3_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b02493-49b5-48fa-a28b-c7576f8384f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T02:52:08.418412Z",
     "iopub.status.busy": "2025-07-09T02:52:08.417663Z",
     "iopub.status.idle": "2025-07-09T02:52:14.315663Z",
     "shell.execute_reply": "2025-07-09T02:52:14.315032Z",
     "shell.execute_reply.started": "2025-07-09T02:52:08.418374Z"
    }
   },
   "outputs": [],
   "source": [
    "upload_dir_to_s3(\"/tmp/finetuned_model_checkpoints\", \"BUCKET_NAME\", \"models/dialogsum/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3a0dfcd-9356-4058-a83e-6312d73bd583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:26:28.231347Z",
     "iopub.status.busy": "2025-07-10T03:26:28.230882Z",
     "iopub.status.idle": "2025-07-10T03:26:34.497355Z",
     "shell.execute_reply": "2025-07-10T03:26:34.496799Z",
     "shell.execute_reply.started": "2025-07-10T03:26:28.231292Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/modeling_utils.py:3465: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Save model locally\n",
    "trainer.save_model(\"./tmp/finetuned_model_checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00986920-9460-401a-ac68-db1f51db38fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:26:34.498525Z",
     "iopub.status.busy": "2025-07-10T03:26:34.498246Z",
     "iopub.status.idle": "2025-07-10T03:26:34.730679Z",
     "shell.execute_reply": "2025-07-10T03:26:34.730242Z",
     "shell.execute_reply.started": "2025-07-10T03:26:34.498499Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"./tmp/finetuned_model_checkpoints\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e2d1a-bf1f-4ddd-8c1c-c58f7656b233",
   "metadata": {},
   "source": [
    "# Manual Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c1349-d5d3-4d7d-9e0e-d50f94c6d693",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:26:34.731418Z",
     "iopub.status.busy": "2025-07-10T03:26:34.731203Z",
     "iopub.status.idle": "2025-07-10T03:26:51.249592Z",
     "shell.execute_reply": "2025-07-10T03:26:51.249030Z",
     "shell.execute_reply.started": "2025-07-10T03:26:34.731400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN BASELINE SUMMARY\n",
      "---------------------------------------------------\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "ORIGINAL MODEL SUMMARY\n",
      "---------------------------------------------------\n",
      "You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. You might also want to add a CD-ROM drive too, because most new software programs are coming out on Cds. It would allow you to make up your own flyers and banners for advertising.\n",
      "---------------------------------------------------\n",
      "FINE TUNED MODEL SUMMARY\n",
      "---------------------------------------------------\n",
      "You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. You might also want to add a CD-ROM drive too, because most new software programs are coming out on Cds. It would allow you to make up your own flyers and banners for advertising.\n"
     ]
    }
   ],
   "source": [
    "# We are going to get the same 200th dialogue from the dataset\n",
    "dialogue = dataset['test'][200]['dialogue']\n",
    "human_baseline_summary = dataset['test'][200]['summary']\n",
    "\n",
    "# Zero shot Inference\n",
    "prompt = f\"\"\"\n",
    "Summarize the following\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "print(\"HUMAN BASELINE SUMMARY\")\n",
    "print(\"---------------------------------------------------\")\n",
    "print(f\"{human_baseline_summary}\" )\n",
    "inputs_to_original_model_encoder = tokenizer(prompt, dialogue, return_tensors=\"pt\")\n",
    "output_from_orginal_model_decoder = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs_to_original_model_encoder[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "\n",
    "print(\"ORIGINAL MODEL SUMMARY\")\n",
    "print(\"---------------------------------------------------\")\n",
    "print(f\"{output_from_orginal_model_decoder}\" )\n",
    "\n",
    "print(\"---------------------------------------------------\")\n",
    "inputs_to_finetuned_model_encoder = tokenizer(prompt, dialogue, return_tensors=\"pt\")\n",
    "output_from_finetuned_model_decoder = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs_to_finetuned_model_encoder[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "print(\"FINE TUNED MODEL SUMMARY\")\n",
    "print(\"---------------------------------------------------\")\n",
    "\n",
    "print(f\"{output_from_finetuned_model_decoder}\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27cfd38-aaca-4d72-8389-574a777b75ca",
   "metadata": {},
   "source": [
    "# Evaluate model with ROUGE metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb754494",
   "metadata": {},
   "source": [
    "ROUGE is a set of metrics commonly used to evaluate text summarisation and machine-generated content. It measures the overlap between the generated text and reference summaries based on n-grams, word sequences, and word pairs.\n",
    "\n",
    "The most commonly used ROUGE variants are:\n",
    "\n",
    "ROUGE-1: Overlap of individual words (unigrams)\n",
    "\n",
    "ROUGE-2: Overlap of word pairs (bigrams)\n",
    "\n",
    "ROUGE-L: Longest Common Subsequence between generated and reference texts\n",
    "\n",
    "Higher ROUGE scores indicate that the model-generated summary is more similar to the human-written one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfc76f24-d710-49d0-8955-7ed9c5670d4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:27:11.107957Z",
     "iopub.status.busy": "2025-07-10T03:27:11.107443Z",
     "iopub.status.idle": "2025-07-10T03:27:12.165754Z",
     "shell.execute_reply": "2025-07-10T03:27:12.165184Z",
     "shell.execute_reply.started": "2025-07-10T03:27:11.107933Z"
    }
   },
   "outputs": [],
   "source": [
    "#pip install rouge_score\n",
    "\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e04f9fb7-0884-4635-8633-9d74e2a62552",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:29:19.215943Z",
     "iopub.status.busy": "2025-07-10T03:29:19.215738Z",
     "iopub.status.idle": "2025-07-10T03:31:24.131780Z",
     "shell.execute_reply": "2025-07-10T03:31:24.131127Z",
     "shell.execute_reply.started": "2025-07-10T03:29:19.215924Z"
    }
   },
   "outputs": [],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "finetuned_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    original_model_outputs = model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "    finetuned_model_outputs = finetuned_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    finetuned_model_text_output = tokenizer.decode(finetuned_model_outputs[0], skip_special_tokens=True)\n",
    "    finetuned_model_summaries.append(finetuned_model_text_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d2e9f-9d8c-494d-90a9-80b603576a5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T03:18:40.406448Z",
     "iopub.status.busy": "2025-07-09T03:18:40.406139Z",
     "iopub.status.idle": "2025-07-09T03:18:40.411460Z",
     "shell.execute_reply": "2025-07-09T03:18:40.410342Z",
     "shell.execute_reply.started": "2025-07-09T03:18:40.406425Z"
    }
   },
   "source": [
    "Evaluate the models computing ROUGE metrics. Notice the improvement in the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03266503-42cd-4dd6-86ec-9ddce653120d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:31:24.133220Z",
     "iopub.status.busy": "2025-07-10T03:31:24.132936Z",
     "iopub.status.idle": "2025-07-10T03:31:24.494218Z",
     "shell.execute_reply": "2025-07-10T03:31:24.493730Z",
     "shell.execute_reply.started": "2025-07-10T03:31:24.133200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.037142857142857144, 'rouge2': 0.005714285714285714, 'rougeL': 0.021525096525096522, 'rougeLsum': 0.021525096525096522}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.2559007555448247, 'rouge2': 0.06673966727231631, 'rougeL': 0.1666196165366397, 'rougeLsum': 0.16674475937860173}\n"
     ]
    }
   ],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "finetuned_model_results = rouge.compute(\n",
    "    predictions=finetuned_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(finetuned_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('FINETUNED MODEL:')\n",
    "print(finetuned_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be870fb1",
   "metadata": {},
   "source": [
    "Notice the improvement in the fine-tuned model’s results. Let’s calculate the absolute percentage of the improvement of the fine-tuned model over original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fed27222-730d-4046-b97b-14df42333bb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:31:24.495295Z",
     "iopub.status.busy": "2025-07-10T03:31:24.494918Z",
     "iopub.status.idle": "2025-07-10T03:31:24.499418Z",
     "shell.execute_reply": "2025-07-10T03:31:24.498864Z",
     "shell.execute_reply.started": "2025-07-10T03:31:24.495263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of FineTuned MODEL over ORIGINAL MODEL\n",
      "rouge1: 21.88%\n",
      "rouge2: 6.10%\n",
      "rougeL: 14.51%\n",
      "rougeLsum: 14.52%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of FineTuned MODEL over ORIGINAL MODEL\")\n",
    "\n",
    "improvement = (np.array(list(finetuned_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(finetuned_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd420d44-66fb-4dc4-969e-cd796a5d27ab",
   "metadata": {},
   "source": [
    "# Perform Parameter Efficient Fine-Tuning (PEFT with LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c61af-cc81-458c-9557-45d5b27423b1",
   "metadata": {},
   "source": [
    "Parameter-Efficient Fine-Tuning (PEFT) with LoRA (Low-Rank Adaptation) is a lightweight technique that allows you to fine-tune large language models (LLMs) by updating only a small subset of parameters. Instead of modifying the entire model, LoRA injects a small number of trainable weights (called low-rank adapters) into specific layers. This approach drastically reduces the number of trainable parameters, making fine-tuning faster, cheaper, and more memory-efficient, while still achieving competitive performance compared to full fine-tuning. Using PEFT/LoRA, you freeze the underlying model layers, add a new adapter layer and train only that. The newly trained layer is then reunited and combined with the original LLM to serve the inference request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c78ce",
   "metadata": {},
   "source": [
    "# Preparing the Model with PEFT and LoRA\n",
    "Configure LoRA configuration. Note the rank parameter (r) controls the size of the low-rank matrices inserted into the model’s layers during fine-tuning.\n",
    "\n",
    "A lower rank (e.g., r=4) means fewer parameters, faster training, less memory – but potentially less expressiveness.\n",
    "\n",
    "A higher rank (e.g., r=64) increases model capacity, but also cost and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "91fd5843-1313-4918-af6c-fc0a1e74cdab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:56:11.098739Z",
     "iopub.status.busy": "2025-07-10T03:56:11.098337Z",
     "iopub.status.idle": "2025-07-10T03:56:11.101960Z",
     "shell.execute_reply": "2025-07-10T03:56:11.101485Z",
     "shell.execute_reply.started": "2025-07-10T03:56:11.098708Z"
    }
   },
   "outputs": [],
   "source": [
    "#pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2c50917b-90aa-4135-b275-8618e513d058",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:56:12.071368Z",
     "iopub.status.busy": "2025-07-10T03:56:12.071051Z",
     "iopub.status.idle": "2025-07-10T03:56:12.074459Z",
     "shell.execute_reply": "2025-07-10T03:56:12.073934Z",
     "shell.execute_reply.started": "2025-07-10T03:56:12.071342Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b328a9d-f62e-4268-9da1-687e621fa102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:32:54.656965Z",
     "iopub.status.busy": "2025-07-10T03:32:54.656248Z",
     "iopub.status.idle": "2025-07-10T03:32:54.659995Z",
     "shell.execute_reply": "2025-07-10T03:32:54.659432Z",
     "shell.execute_reply.started": "2025-07-10T03:32:54.656939Z"
    }
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f43da-2608-4e8e-bb77-4e28b846204a",
   "metadata": {},
   "source": [
    "Add LoRA adapter layer to the original model to train it, and print the trainable parameters. You would see that the number of trainable parameters will be much smaller in this case. Only 1.15% of the total parameters are trainable in the adapter layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f85ab242-44da-4467-a969-c4641a86137d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:32:56.043595Z",
     "iopub.status.busy": "2025-07-10T03:32:56.042936Z",
     "iopub.status.idle": "2025-07-10T03:32:56.152560Z",
     "shell.execute_reply": "2025-07-10T03:32:56.152067Z",
     "shell.execute_reply.started": "2025-07-10T03:32:56.043573Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86030283-c641-490e-bf30-b4b910e95b22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:32:59.254769Z",
     "iopub.status.busy": "2025-07-10T03:32:59.254027Z",
     "iopub.status.idle": "2025-07-10T03:32:59.261142Z",
     "shell.execute_reply": "2025-07-10T03:32:59.260568Z",
     "shell.execute_reply.started": "2025-07-10T03:32:59.254743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 411009024, \n",
      " Total Trainable Parameters are: 4718592\n"
     ]
    }
   ],
   "source": [
    "print(number_of_trainable_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1d8bd8-e52b-45ab-a12f-f853f68aaf6f",
   "metadata": {},
   "source": [
    "## Train PEFT layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950d411a",
   "metadata": {},
   "source": [
    "Similar to Full fine-tuning, we will create training arguments and an instance of the Trainer class from Hugging Face.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6fe8aad5-1ffc-4d56-be30-2d362f6e006a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T03:54:02.193847Z",
     "iopub.status.busy": "2025-07-10T03:54:02.193195Z",
     "iopub.status.idle": "2025-07-10T03:54:02.198146Z",
     "shell.execute_reply": "2025-07-10T03:54:02.197568Z",
     "shell.execute_reply.started": "2025-07-10T03:54:02.193821Z"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from transformers import Trainer\n",
    "\n",
    "class SafeTrainer(Trainer):\n",
    "    def training_step(self, model, inputs, optimizer=None):\n",
    "        model.train()\n",
    "\n",
    "        # Filter out invalid keys not accepted by model.forward\n",
    "        valid_keys = inspect.signature(model.forward).parameters\n",
    "        filtered_inputs = {k: v for k, v in inputs.items() if k in valid_keys}\n",
    "\n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, filtered_inputs)\n",
    "\n",
    "        # Normalize loss for gradient accumulation\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4d1e1845-2f8d-47c2-b390-fa12ffb99dc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T04:04:19.918061Z",
     "iopub.status.busy": "2025-07-10T04:04:19.917731Z",
     "iopub.status.idle": "2025-07-10T04:04:19.922450Z",
     "shell.execute_reply": "2025-07-10T04:04:19.921952Z",
     "shell.execute_reply.started": "2025-07-10T04:04:19.918039Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = \"/tmp/peft_model_checkpoints\"\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    max_steps=1,\n",
    "    save_safetensors=False\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "daf137bf-de36-4147-ba90-e5e60679915d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T04:04:21.417880Z",
     "iopub.status.busy": "2025-07-10T04:04:21.417557Z",
     "iopub.status.idle": "2025-07-10T04:04:21.432290Z",
     "shell.execute_reply": "2025-07-10T04:04:21.431837Z",
     "shell.execute_reply.started": "2025-07-10T04:04:21.417858Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_trainer = SafeTrainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_training_datasets['train'])\n",
    "   # eval_dataset=tokenized_training_datasets['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f7880d",
   "metadata": {},
   "source": [
    "Train and save the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "de98c503-3d22-4a14-827b-ec8c106bd931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T04:04:24.133109Z",
     "iopub.status.busy": "2025-07-10T04:04:24.132774Z",
     "iopub.status.idle": "2025-07-10T04:05:09.943051Z",
     "shell.execute_reply": "2025-07-10T04:05:09.942577Z",
     "shell.execute_reply.started": "2025-07-10T04:04:24.133087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11.125000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b110a3b1fd947a4b5ab31030114654f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "peft_trainer.model.save_pretrained(\"./tmp/peft_model_checkpoints\", safe_serialization=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8255746c",
   "metadata": {},
   "source": [
    "Optionally, you can save the model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1ced0-2330-4a7b-a92b-8eade6aeab90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T04:16:33.658971Z",
     "iopub.status.busy": "2025-07-10T04:16:33.658409Z",
     "iopub.status.idle": "2025-07-10T04:16:36.901538Z",
     "shell.execute_reply": "2025-07-10T04:16:36.901029Z",
     "shell.execute_reply.started": "2025-07-10T04:16:33.658950Z"
    }
   },
   "outputs": [],
   "source": [
    "upload_dir_to_s3(\"/tmp/peft_model_checkpoints\", \"BUCKET_NAME\", \"peft/models/dialogsum/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "92b3a975-0590-479e-a55b-b380c7bb41b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T04:06:50.628387Z",
     "iopub.status.busy": "2025-07-10T04:06:50.628050Z",
     "iopub.status.idle": "2025-07-10T04:07:00.296661Z",
     "shell.execute_reply": "2025-07-10T04:07:00.296002Z",
     "shell.execute_reply.started": "2025-07-10T04:06:50.628363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d885e28d12f479696c86b4fcda79aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04b73a0f51f4ddfb5372d2e5f830b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf055c2f3304fe282a7b1fb4669a4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f95927844a74358a0090d340f4c6a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e6df1bffd9411eac8d19a6605a20c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       './tmp/peft_model_checkpoints/', \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d720418a",
   "metadata": {},
   "source": [
    "The is_trainable flag is set to False to freeze the model, indicating that no further training will be performed. As a result, the number of trainable parameters is now zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d7739159-7489-47b4-83d4-6b306e8d1406",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T04:07:17.898194Z",
     "iopub.status.busy": "2025-07-10T04:07:17.897874Z",
     "iopub.status.idle": "2025-07-10T04:07:17.904344Z",
     "shell.execute_reply": "2025-07-10T04:07:17.903787Z",
     "shell.execute_reply.started": "2025-07-10T04:07:17.898170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 411009024, \n",
      " Total Trainable Parameters are: 0\n"
     ]
    }
   ],
   "source": [
    "print(number_of_trainable_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10582ea5-4a8f-45d4-9800-e203c3fff891",
   "metadata": {},
   "source": [
    "# Evaluate the Model Manually\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dff7d3",
   "metadata": {},
   "source": [
    "We are testing the 200th dialogue as in the previous section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ba3bbc5b-21e5-46f9-b177-b37f611b8bdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T04:10:22.337433Z",
     "iopub.status.busy": "2025-07-10T04:10:22.337112Z",
     "iopub.status.idle": "2025-07-10T04:10:38.776544Z",
     "shell.execute_reply": "2025-07-10T04:10:38.775803Z",
     "shell.execute_reply.started": "2025-07-10T04:10:22.337412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "The conversation goes as follows: #Person1#: Have you considered upgrading your system? #Person2#: Yes, but I'm not sure what exactly I would need. #Person1#: You could consider adding a painting program to your software. #Person2#: That would be a definite bonus.\n",
      "---------------------------------------\n",
      "FULL FINETUNED MODEL:\n",
      "The conversation took place between a man and his son. The man asked his son if he would like to upgrade his computer. The son said he would need a faster processor, a more powerful hard disc, more memory and a faster modem. The pair also discussed the possibility of adding a painting program to their software.\n",
      "---------------------------------------\n",
      "PEFT MODEL: . The conversation with a friend. The conversation took place in a computer lab.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][200]['dialogue']\n",
    "human_baseline_summary = dataset['test'][200]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "finetuned_model_outputs = finetuned_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "finetuned_model_text_output = tokenizer.decode(finetuned_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(\"---------------------------------------\")\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(\"---------------------------------------\")\n",
    "print(f'FULL FINETUNED MODEL:\\n{finetuned_model_text_output}')\n",
    "print(\"---------------------------------------\")\n",
    "print(f'PEFT MODEL: {peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b201b5f-39da-4233-9ef7-2d500a5dd647",
   "metadata": {},
   "source": [
    "# Evaluate Model with ROUGE Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "11660e08-c121-43de-a1cc-62641678d92c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T04:11:58.982796Z",
     "iopub.status.busy": "2025-07-10T04:11:58.982207Z",
     "iopub.status.idle": "2025-07-10T04:14:55.768263Z",
     "shell.execute_reply": "2025-07-10T04:14:55.767625Z",
     "shell.execute_reply.started": "2025-07-10T04:11:58.982768Z"
    }
   },
   "outputs": [],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "finetuned_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    \n",
    "    original_model_outputs = model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    finetuned_model_outputs = finetuned_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    finetuned_model_text_output = tokenizer.decode(finetuned_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    finetuned_model_summaries.append(finetuned_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5310c6-13c2-4120-8376-055ef60e92fb",
   "metadata": {},
   "source": [
    "Compute ROUGE score for this subset of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0f774812-cf25-4f9c-a47a-2f729109e933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T04:14:55.981031Z",
     "iopub.status.busy": "2025-07-10T04:14:55.980597Z",
     "iopub.status.idle": "2025-07-10T04:14:57.790010Z",
     "shell.execute_reply": "2025-07-10T04:14:57.789488Z",
     "shell.execute_reply.started": "2025-07-10T04:14:55.981002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96facaebd3e145cdbe0cfd17ca540548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.20441034936319014, 'rouge2': 0.043375890362790576, 'rougeL': 0.14238471172139486, 'rougeLsum': 0.14018566815697964}\n",
      "FINETUNED MODEL:\n",
      "{'rouge1': 0.2559007555448247, 'rouge2': 0.06673966727231631, 'rougeL': 0.1666196165366397, 'rougeLsum': 0.16674475937860173}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.037142857142857144, 'rouge2': 0.005714285714285714, 'rougeL': 0.021525096525096522, 'rougeLsum': 0.021525096525096522}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "finetuned_model_results = rouge.compute(\n",
    "    predictions=finetuned_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(finetuned_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('FINETUNED MODEL:')\n",
    "print(finetuned_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f51169",
   "metadata": {},
   "source": [
    "Let’s calculate the percentage of improvement of PEFT over original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c2896fb2-76d0-4862-957f-faaaa810555f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T04:14:57.797774Z",
     "iopub.status.busy": "2025-07-10T04:14:57.797537Z",
     "iopub.status.idle": "2025-07-10T04:14:57.801862Z",
     "shell.execute_reply": "2025-07-10T04:14:57.801288Z",
     "shell.execute_reply.started": "2025-07-10T04:14:57.797756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\n",
      "rouge1: -16.73%\n",
      "rouge2: -3.77%\n",
      "rougeL: -12.09%\n",
      "rougeLsum: -11.87%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02698368",
   "metadata": {},
   "source": [
    "While the PEFT (LoRA) model shows a slight decrease in ROUGE metrics compared to the fully fine-tuned model, this trade-off is expected and often acceptable. The small drop in performance is compensated by significant efficiency gains — LoRA trains only a small fraction of the model’s parameters, drastically reducing memory usage, compute requirements, and training time.\n",
    "\n",
    "This means you can fine-tune large models on a single GPU or limited hardware, making it ideal for quick experimentation or resource-constrained environments. In many practical applications, this minor performance difference is negligible when weighed against the cost and scalability benefits of PEFT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
