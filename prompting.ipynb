{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dead8f03-5a88-4472-a8ad-b0e6aaeba78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anoopmishra/Repositories/anoop/HuggingFace/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8abae639-994a-4fa1-8259-4999fd4321d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the same dataset again\n",
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67a8313-906a-4b2e-86c8-2dc748d3563c",
   "metadata": {},
   "source": [
    "See what this dialogues contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "717ba6b9-b7d8-4666-8c19-8c5e70312ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: #Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "Human Summary: In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n"
     ]
    }
   ],
   "source": [
    "print(f\"dialogue: {dataset['test'][1]['dialogue']}\")\n",
    "print(f\"Human Summary: {dataset['test'][1]['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80605b65-f65b-4bd9-9677-9622e55f3457",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6478f0af-4bc4-49f4-8e9f-cf17878925c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af55973a-e9bc-43e4-8abf-8d7ab1a88ea4",
   "metadata": {},
   "source": [
    "Now it's time to explore how well the base LLM summarizes a dialogue without any prompt engineering. **Prompt engineering** is an act of a human changing the **prompt** (input) to improve the response for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a9559cf-06eb-4dd5-b875-7cd6e4322a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: #Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary: In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Summary - without prompt engineering : #Person1#: Ms. Dawson, I need you to take a dictation for me.\n"
     ]
    }
   ],
   "source": [
    "dash_line = '-'.join('' for x in range(100))\n",
    "dialog = dataset['test'][1]['dialogue']\n",
    "summary = dataset['test'][1]['summary']\n",
    "\n",
    "inputs_to_encoder = tokenizer(dialog, return_tensors=\"pt\")\n",
    "output_from_decoder = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs_to_encoder[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "print(f\"dialogue: {dataset['test'][1]['dialogue']}\")\n",
    "print(dash_line)\n",
    "print(f\"Human Summary: {dataset['test'][1]['summary']}\")\n",
    "print(dash_line)\n",
    "print(f\"Model Summary - without prompt engineering : {output_from_decoder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1852d4-e2ce-410b-b694-7784b4842846",
   "metadata": {},
   "source": [
    "# Zero Shot inference with instruction prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59acf7d9-f089-40c2-b7a1-17afdcd0722e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: #Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary: In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Summary - Zero Shot : #Person1#: I need to take a dictation for you.\n"
     ]
    }
   ],
   "source": [
    "dash_line = '-'.join('' for x in range(100))\n",
    "dialog = dataset['test'][1]['dialogue']\n",
    "summary = dataset['test'][1]['summary']\n",
    "\n",
    "instruction_prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialog}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "inputs_to_encoder = tokenizer(instruction_prompt, return_tensors=\"pt\")\n",
    "output_from_decoder = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs_to_encoder[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "print(f\"dialogue: {dataset['test'][1]['dialogue']}\")\n",
    "print(dash_line)\n",
    "print(f\"Human Summary: {dataset['test'][1]['summary']}\")\n",
    "print(dash_line)\n",
    "print(f\"Model Summary - Zero Shot : {output_from_decoder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed12fa3-b0f2-4239-9373-4f3cb04a855e",
   "metadata": {},
   "source": [
    "This is much better but still a lot of nuance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70629f22-248b-475b-8768-26d3bf3a89c4",
   "metadata": {},
   "source": [
    "## One Shot Inference\n",
    "It's a way of providing LLMs with one example of prompt-response pair that match your task to give LLM some context. For example:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4605208e-9ebc-449e-8d22-bfec4cfba656",
   "metadata": {},
   "source": [
    "Translate English to French:\n",
    "\n",
    "English: Hello\n",
    "French: Bonjour\n",
    "\n",
    "English: Good night\n",
    "French:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11663d92-499c-45e7-8000-eee051d5d0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(index_for_prompt, index_to_summarise):\n",
    "    prompt = ''\n",
    "    for index in index_for_prompt:\n",
    "        dialog = dataset['test'][index_for_prompt]['dialogue']\n",
    "        summary = dataset['test'][index_for_prompt]['summary']\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Dialogue:\n",
    "        \n",
    "        {dialog}\n",
    "        \n",
    "        What was going on?\n",
    "        {summary}\n",
    "        \n",
    "            \"\"\" \n",
    "\n",
    "    dialogue = dataset['test'][index_to_summarise]['dialogue']\n",
    "    \n",
    "    prompt += f\"\"\"\n",
    "    Dialogue:\n",
    "    \n",
    "    {dialogue}\n",
    "    \n",
    "    What was going on?\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db862e2-08ce-4e84-b992-a0294b1bcfce",
   "metadata": {},
   "source": [
    "Construct One shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e38e9d0-0c99-4657-8e7c-e6c119992565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Dialogue:\n",
      "\n",
      "        [\"#Person1#: What time is it, Tom?\\n#Person2#: Just a minute. It's ten to nine by my watch.\\n#Person1#: Is it? I had no idea it was so late. I must be off now.\\n#Person2#: What's the hurry?\\n#Person1#: I must catch the nine-thirty train.\\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\"]\n",
      "\n",
      "        What was going on?\n",
      "        ['#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.']\n",
      "\n",
      "            \n",
      "    Dialogue:\n",
      "\n",
      "    #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "    What was going on?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "one_shot_prompt = make_prompt([40], 200)\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26c9e3f8-9f15-4e1e-a680-ae5501d3c8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Summary - One Shot Inference : #Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n"
     ]
    }
   ],
   "source": [
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "\n",
    "inputs_to_encoder = tokenizer(one_shot_prompt, return_tensors=\"pt\")\n",
    "output_from_decoder = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs_to_encoder[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(f\"Human Summary: {dataset['test'][200]['summary']}\")\n",
    "print(dash_line)\n",
    "print(f\"Model Summary - One Shot Inference : {output_from_decoder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dae25d-792d-4b67-bc7e-7ba7442945ed",
   "metadata": {},
   "source": [
    "# Few Shot Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a07951c-345c-4a86-8618-616e6646aba3",
   "metadata": {},
   "source": [
    "It's a way of providing LLMs with a few examples of prompt-response pairs typically 2-5 of a task within the prompt to help it understand the pattern before performing on new inputs. For example:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81ab75fb-be3f-4f18-8470-43b17cc61c72",
   "metadata": {},
   "source": [
    "Classify the sentiment as Positive or Negative:\n",
    "\n",
    "Text: I love this product!\n",
    "Sentiment: Positive\n",
    "\n",
    "Text: This was the worst experience ever.\n",
    "Sentiment: Negative\n",
    "\n",
    "Text: The service was great, and I'll come back again.\n",
    "Sentiment:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f753ef-0e74-47d4-a353-d6fd1a127e62",
   "metadata": {},
   "source": [
    "In the previous example, we gave model one example with index 40. In this example, we will provide LLMs with 3 examples with index 40, 50,  and 60."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fb9801-c266-455c-a6b4-2ac6ba14062b",
   "metadata": {},
   "source": [
    "Construct Few shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b50ea08f-c26c-4793-b9e0-42d8eb8b100d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Dialogue:\n",
      "\n",
      "        [\"#Person1#: What time is it, Tom?\\n#Person2#: Just a minute. It's ten to nine by my watch.\\n#Person1#: Is it? I had no idea it was so late. I must be off now.\\n#Person2#: What's the hurry?\\n#Person1#: I must catch the nine-thirty train.\\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\", \"#Person1#: Yeah. Just pull on this strip. Then peel off the back.\\n#Person2#: You might make a few enemies this way.\\n#Person1#: If they don't think this is fun, they're not meant to be our friends.\\n#Person2#: You mean your friends. I think it's cruel.\\n#Person1#: Yeah. But it's fun. Look at those two ugly old ladies. . . or are they men?\\n#Person2#: Hurry! Get a shot!. . . Hand it over!\\n#Person1#: I knew you'd come around. . .\", \"#Person1#: Hey, Frank. I heard you got a new job.\\n#Person2#: Yeah, Judy. I will be working for the Post Office. It's not a bad job.\\n#Person1#: Is it true that you have a heavy work schedule?\\n#Person2#: That's right. I am supposed to work at 5am everyday, and I only get 45 minutes for lunch.\\n#Person1#: So, why did you apply for such a demanding job?\\n#Person2#: Well, the government offers its employees excellent health insurance benefits.\\n#Person1#: Oh, I see. And can your family members utilize the health insurance, too?\\n#Person2#: Yeah, that's the best part. All of my children can get free medical care.\\n#Person1#: That's a great employment benefit!\\n#Person2#: Now you know why I wanted to work for the Post Office!\"]\n",
      "\n",
      "        What was going on?\n",
      "        ['#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.', \"#Person1# is about to make a prank. #Person2# thinks it's cruel at first but then joins.\", 'Frank got a new job and is telling Judy not only the heavy schedule but also the benefits of this job.']\n",
      "\n",
      "            \n",
      "    Dialogue:\n",
      "\n",
      "    #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "    What was going on?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = make_prompt([40,50,60], 200)\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc4ed7ea-0083-4c54-905d-85f6942f2c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Summary - Few Shot Inference : #Person1 wants to upgrade his system and hardware.\n"
     ]
    }
   ],
   "source": [
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "\n",
    "inputs_to_encoder = tokenizer(few_shot_prompt, return_tensors=\"pt\")\n",
    "output_from_decoder = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs_to_encoder[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(f\"Human Summary: {dataset['test'][200]['summary']}\")\n",
    "print(dash_line)\n",
    "print(f\"Model Summary - Few Shot Inference : {output_from_decoder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23127848-90e0-4f05-a34e-99bc31203092",
   "metadata": {},
   "source": [
    "## Experiment GenerateConfig parameters for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b81080-87db-42f2-9fc2-2e131580fe73",
   "metadata": {},
   "source": [
    "### Adjust max_new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f863ae54-c5db-416b-971b-6ae14bfb2bfc",
   "metadata": {},
   "source": [
    "In this example, I am reducing the max_new_tokens value from 200 to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9de988ca-ea72-4a77-a747-74bb60e99791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Summary - Few Shot Inference : #Person1 wants to upgrade his system and hardware.\n"
     ]
    }
   ],
   "source": [
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "\n",
    "inputs_to_encoder = tokenizer(few_shot_prompt, return_tensors=\"pt\")\n",
    "output_from_decoder = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs_to_encoder[\"input_ids\"], \n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(f\"Human Summary: {dataset['test'][200]['summary']}\")\n",
    "print(dash_line)\n",
    "print(f\"Model Summary - Few Shot Inference : {output_from_decoder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41527714-3b7d-429a-a67c-953e8c7311a5",
   "metadata": {},
   "source": [
    "### Add temperature parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9319cc-4964-43b8-a903-750ff3e9a7a6",
   "metadata": {},
   "source": [
    "temperature is a floating-point value (typically between 0 and 1, but sometimes up to 2), used during sampling from the model’s probability distribution over words/tokens.\n",
    "\n",
    "Lower temperature (e.g. 0 – 0.3)\n",
    "→ More deterministic, conservative output\n",
    "→ Picks the most likely tokens\n",
    "→ Good for facts, coding, structured tasks\n",
    "→ Reproducible results\n",
    "\n",
    "Medium temperature (e.g. 0.5 – 0.7)\n",
    "→ Balanced between randomness and coherence\n",
    "→ Suitable for brainstorming or more open-ended tasks\n",
    "\n",
    "Higher temperature (e.g. 0.8 – 1.0 or higher)\n",
    "→ More diverse and creative output\n",
    "→ Can produce surprising or novel responses\n",
    "→ Risk of hallucinations or incoherence increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "13ee1799-90d3-48a6-bc9b-1fb653d433cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Summary - Few Shot Inference : #Person1 recommends upgrading the system, adding a painting program, adding a computer and a CD-ROM drive.\n"
     ]
    }
   ],
   "source": [
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "\n",
    "inputs_to_encoder = tokenizer(few_shot_prompt, return_tensors=\"pt\")\n",
    "output_from_decoder = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs_to_encoder[\"input_ids\"], \n",
    "        max_new_tokens=50, do_sample=True, temperature=0.1\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(f\"Human Summary: {dataset['test'][200]['summary']}\")\n",
    "print(dash_line)\n",
    "print(f\"Model Summary - Few Shot Inference : {output_from_decoder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e3f07-3646-4d3a-9de3-f9fa453dee67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
